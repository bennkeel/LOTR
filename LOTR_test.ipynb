{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Josh Blaz -- LOTR\n",
    "## CS401 -- NLP\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import urllib.request\n",
    "import lxml.html as lh\n",
    "import io\n",
    "import requests\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter \n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "#NOTE: Sentiment140 Polarity values: 0: negative, 2: neutral, 4: positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE: Elvish text is translated awkwardly into the .txt format    \n",
    "</br>\n",
    "##### IE:   \n",
    "</br>\n",
    "►M MPR -F+MTRX MP ft PPtK P&RMPht: P. t. The last Two runes are the initials of Thror and Thrain.**  \n",
    "</br>\n",
    "#### Same with some of the intros:\n",
    "</br>\n",
    "“THE LORD OF THE RINGS” \n",
    "\n",
    "Pjrt Thttt \n",
    "\n",
    "THE RETURN \n",
    "OF THE KING \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get all tokens\n",
    "\n",
    "## --- The Silmarillion ---\n",
    "silm_file = open('silmarillion.txt', 'r')\n",
    "silm = silm_file.read() \n",
    "silm_raw = silm[43190:-5436] ## save raw files for later\n",
    "silm = silm.lower() ## Make all words lowercase \n",
    "silm = silm[43190:-5436] ## remove HTML jargon\n",
    "silm = nltk.word_tokenize(silm) ## tokenize\n",
    "\n",
    "## --- The Hobbit ---\n",
    "hobbit_file = open('hobbit.txt', 'r')\n",
    "hobbit = hobbit_file.read()\n",
    "hobbit_raw = hobbit[43212:-8543]\n",
    "hobbit = hobbit.lower()\n",
    "hobbit = hobbit[43212:-8543]\n",
    "hobbit = nltk.word_tokenize(hobbit)\n",
    "\n",
    "## --- The Fellowship of the Ring ---\n",
    "fellowship_file = open('fellowship.txt', 'r')\n",
    "fellowship = fellowship_file.read()\n",
    "fellowship_raw = fellowship[43242:-5436]\n",
    "fellowship = fellowship.lower()\n",
    "fellowship = fellowship[43242:-5436]\n",
    "fellowship = nltk.word_tokenize(fellowship)\n",
    "\n",
    "## --- The Two Towers ---\n",
    "twotowers_file = open('twotowers.txt', 'r')\n",
    "twotowers = twotowers_file.read()\n",
    "twotowers_raw = twotowers[43302:-19245]\n",
    "twotowers = twotowers.lower()\n",
    "twotowers = twotowers[43302:-19245]\n",
    "twotowers = nltk.word_tokenize(twotowers)\n",
    "\n",
    "## --- The Return of the King ---\n",
    "ret_file = open('return.txt', 'r')\n",
    "ret = ret_file.read()\n",
    "ret_raw = ret[43252:-5434]\n",
    "ret = ret.lower()\n",
    "ret = ret[43252:-5434]\n",
    "ret = nltk.word_tokenize(ret)\n",
    "\n",
    "tokenlist = [silm, hobbit, fellowship, twotowers, ret]\n",
    "\n",
    "raw_texts = [silm_raw, hobbit_raw, fellowship_raw, twotowers_raw, ret_raw] #Text files to use with LDA topic modeling\n",
    "\n",
    "\n",
    "entirety = [] ## This is all tokens combined\n",
    "for i in range(len(tokenlist)):\n",
    "    for word in tokenlist[i]:\n",
    "        entirety.append(word)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Used Chapterize to split books into chapters \n",
    "## https://github.com/JonathanReeve/chapterize\n",
    "### Chapterize didn't work 100% perfectly, so I had to go through and the prologues back in when it cut them out\n",
    "\n",
    "# These are lists containing strings of every chapter for each book\n",
    "silm_chapters = []\n",
    "hobbit_chapters = []\n",
    "fellowship_chapters = []\n",
    "twotowers_chapters = []\n",
    "return_chapters = []\n",
    "\n",
    "# Paths to directories storing book chapters\n",
    "list_of_paths = ['/Users/blaz/Desktop/LOTR/silmarillion-chapters', '/Users/blaz/Desktop/LOTR/hobbit-chapters',\\\n",
    "                '/Users/blaz/Desktop/LOTR/fellowship-chapters', '/Users/blaz/Desktop/LOTR/twotowers-chapters',\\\n",
    "                '/Users/blaz/Desktop/LOTR/return-chapters']\n",
    "\n",
    "for path in list_of_paths: # iterate through the list of folder paths for each book\n",
    "    for file in sorted(glob.glob(os.path.join(path,'*.txt'))): # This gives us a sorted list of the files in each directory                                                         \n",
    "        f = open(file, 'r') # open and read file               # allowing us to read in the chapters in order.\n",
    "        txt = f.read()\n",
    "        ## determine which path we're using and append it to the correct book chapter list\n",
    "        if path == '/Users/blaz/Desktop/LOTR/silmarillion-chapters': \n",
    "            # Because of an issue with 'glob', I had to create a copy of the final chapter in The Silmarillion\n",
    "            silm_chapters.append(txt)\n",
    "        elif path == '/Users/blaz/Desktop/LOTR/hobbit-chapters':\n",
    "            hobbit_chapters.append(txt)\n",
    "        elif path == '/Users/blaz/Desktop/LOTR/fellowship-chapters': \n",
    "            fellowship_chapters.append(txt)\n",
    "        elif path == '/Users/blaz/Desktop/LOTR/twotowers-chapters': \n",
    "            twotowers_chapters.append(txt)\n",
    "        elif path == '/Users/blaz/Desktop/LOTR/return-chapters': \n",
    "            return_chapters.append(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store chapter names for use in dataframes later\n",
    "\n",
    "silm_chapter_names = [\"Ainundalë\", \"Valaquenta\", \"Of the Beginning of Days\", \"Of Aulë and Yavanna\" , \"Of the Coming of the Elves and the Captivity of Melkor\",\\\n",
    "                     \"Of Thingol and Melian\", \"Of Eldamar and the Princes of the Eldalië\", \"Of Fëanor and the Unchaining of Melkor\", \"Of the Silmarils and the Unrest of the Noldor\",\\\n",
    "                     \"Of the Darkening of Valinor\", \"Of the Flight of the Noldor\", \"Of the Sindar\", \"Of the Sun and Moon and the Hiding of Valinor\", \"Of Men\", \"Of the Return of the Noldor\",\\\n",
    "                     \"Of Beleriad and its Realms\", \"Of the Noldor in Beleriad\", \"Of Maeglin\", \"Of the Coming of Men into the West\", \"Of the Ruin of Beleriad and the Fall of Fingolfin\", \"Of Beren and Lúthien\",\\\n",
    "                     \"Of the Fifth Battle: Nirnaeth Arnoediad\", \"Of Túrin Turambar\", \"Of the Ruin of Doriath\", \"Of Tuor and the Fall of Gondolin\", \"Of the Voyage of Eärendil and the War of Wrath\", \\\n",
    "                     \"Akallabêth: The Downfall of Númenor\", \"Of the Rings of Power and the Third Age\"]\n",
    "\n",
    "hobbit_chapter_names = [\"An Unexpected Party\", \"Roast Mutton\", \"A Short Rest\", \"Over Hill and Under Hill\", \"Riddles In The Dark\", \\\n",
    "                       \"Out Of The Frying-Pan Into The Fire\", \"Queer Lodgings\", \"Flies And Spiders\", \"Barrels Out Of Bond\", \"A Warm Welcome\", \\\n",
    "                       \"On The Doorstep\", \"Inside Information\", \"Not At Home\", \"Fire And Water\", \"The Gathering Of The Clouds\", \"A Thief In The Night\", \\\n",
    "                       \"The Clouds Burst\", \"The Return Journey\", \"The Last Stage\"]\n",
    "\n",
    "fellowship_chapter_names = [\"Concerning Hobbits\", \"Concerning Pipeweed\", \"Of the Ordering of the Shire\", \"Note on the Shire Records\", \"A Long-expected Party\", \"The Shadow of the Past\", \\\n",
    "                           \"Three is Company\", \"A Short Cut to Mushrooms\", \"A Conspiracy Unmasked\", \"The Old Forest\", \"In the House of Tom Bombadil\", \"Fog on the Barrow-downs\", \"At the Sign of the Prancing Pony\",\\\n",
    "                           \"Strider\", \"A Knife in the Dark\", \"Flight to the Ford\", \"Many Meetings\", \"The Council of Elrond\", \"The Ring goes South\", \"A Journey in the Dark\", \"The Bridge of Khazad-dûm\", \\\n",
    "                           \"Lothlórien\", \"The Mirror of Galadriel\", \"Farewell to Lórien\", \"The Great River\", \"The Breaking of the Fellowship\"]\n",
    "\n",
    "twotowers_chapter_names = [\"The Departure of Boromir\", \"The Riders of Rohan\", \"The Uruk-hai\", \"Treebeard\", \"The White Rider\", \"The King of the Golden Hall\", \"Helm's Deep\", \"The Road to Isengard\", \"Flotsam and Jetsam\", \\\n",
    "                          \"The Voice of Saruman\", \"The Palantír\", \"The Taming of Smeagol\", \"The Passage of the Marshes\", \"The Black Gate is Closed\", \"Of Herbs and Stewed Rabbit\", \"The Window of the West\", \"The Forbidden Pool\", \\\n",
    "                          \"Journey to the Cross-roads\", \"The Stairs to Cirith Ungol\", \"Shelob's Lair\", \"The Choices of Master Samwise\"]\n",
    "\n",
    "return_chapter_names = [\"Minas Tirith\", \"The Passing of the Grey Company\", \"The Muster of Rohan\", \"The Siege of Gondor\", \"The Ride of Rohirrim\", \"The Battle of the Pelennor Fields\", \"The Pyre of Denethor\",\\\n",
    "                       \"The Houses of Healing\", \"The Last Debate\", \"The Black Gate Opens\", \"The Tower of Cirith Ungol\", \"The Land of Shadow\", \"Mount Doom\", \"The Field of Cormallen\", \"The Steward and the King\", \\\n",
    "                       \"Many Partings\", \"Homeward Bound\", \"Scouring of the Shire\", \"The Grey Havens\"]\n",
    "\n",
    "chapter_name_list = [silm_chapter_names, hobbit_chapter_names, fellowship_chapter_names, twotowers_chapter_names, return_chapter_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### ** 1. Segment all chapters into page-sized objects    **    \n",
    "</br>\n",
    "### ** 2. Send all segments to Sentiment140 API by chapter    **   \n",
    "</br>\n",
    "### ** 3. Calculate polarity averages and polarity lists. **   \n",
    "</br>\n",
    "### ** 4. Store API polarity ratings and export to csv**\n",
    "</br>\n",
    "### ** 5. Plot all polarities + Averages**       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function that segments given chapter into n-sized segments to be sent to the API.\n",
    "Typically using n=2940, as this is the #chars in my copy of Fellowship of the Ring.\n",
    "\n",
    "Parameters - chapter - chapter of a book to be broken into segments\n",
    "           - n - length that we segment the text with\n",
    "       \n",
    "Returns a list of (string) segments of the chapter.\n",
    "\"\"\"\n",
    "def Segmenter(chapter, n):\n",
    "    segments = []\n",
    "    # start and end indices for segmenting the text\n",
    "    start = 0\n",
    "    end = n\n",
    "    while end < len(chapter) + n:\n",
    "        segments.append(chapter[start:end])\n",
    "        start = end\n",
    "        end = end + n\n",
    "    return segments #segments of input chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Goal:\n",
    "# Create lists of lists for segments of each chapter of each book, append to them using \"Segmenter\" function, \n",
    "# storing them like this will allow for iterative querying of the API server\n",
    "\n",
    "\n",
    "# Lists of Lists of Lists storing all segments of all chapters for each book\n",
    "# [[chapter1 segment 0-2500, chap1, segmenet 2500-5000]... [chapter2 segment0-2500, ...]...]\n",
    "silm_segments = []\n",
    "hobbit_segments = []\n",
    "fellowship_segments = []\n",
    "twotowers_segments = []\n",
    "return_segments = []\n",
    "\n",
    "# List containing the lists storing each books' chapters\n",
    "list_of_books = [silm_chapters, hobbit_chapters, fellowship_chapters, twotowers_chapters, return_chapters]\n",
    "# List allowing us to access the segment lists\n",
    "list_of_segments = [silm_segments, hobbit_segments, fellowship_segments, twotowers_segments, return_segments]\n",
    "\n",
    "for i in range(len(list_of_books)):\n",
    "    for chapter in list_of_books[i]: # Segment entire chapter using Segmenter function, with 2940 character cuts\n",
    "        list_of_segments[i].append(Segmenter(chapter,2940))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function that sends segments of 1 chapter through the Sentiment140 API.\n",
    "In order to do so, it creates and appends segments to a JSON file, then posts the JSON queries to the API server\n",
    "using requests module (using an HTTP Post)\n",
    "\n",
    "Parameters - chapter_segments - segments of an entire chapter of a book\n",
    "\n",
    "Returns a list of polarities for segments of the chapter, as well as the polarity average for the chapter\n",
    "\n",
    "Note: Maximum of 700,000 characters per API request, though this shouldn't be a problem\n",
    "\"\"\"\n",
    "\n",
    "def Polarity(chapter_segments): # segments of a single chapter\n",
    "    request = {'data':[]}\n",
    "    polarityList = []\n",
    "    counter = 0\n",
    "    for segment in chapter_segments: # Fill JSON\n",
    "        request['data'].append({'text':segment})\n",
    "    r = requests.post('http://www.sentiment140.com/api/bulkClassifyJson?appid=blaz_j1@denison.edu', json=request)\n",
    "    jso = r.json()\n",
    "    for i in range(len(request['data'])-1):\n",
    "        polarityList.append(jso['data'][i]['polarity'])\n",
    "    \n",
    "    polarityTotal = 0\n",
    "    for value in polarityList:\n",
    "        polarityTotal = polarityTotal + value\n",
    "    \n",
    "    polarityAVG = polarityTotal/len(polarityList)\n",
    "    return polarityList, polarityAVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# store all averages, then store chapter avg, also overall average\n",
    "silm_polarity_avg = []\n",
    "hobbit_polarity_avg = []\n",
    "fellowship_polarity_avg = []\n",
    "twotowers_polarity_avg = []\n",
    "return_polarity_avg = []\n",
    "\n",
    "silm_polarity_lists = []\n",
    "hobbit_polarity_lists = []\n",
    "fellowship_polarity_lists = []\n",
    "twotowers_polarity_lists = []\n",
    "return_polarity_lists = []\n",
    "### Need to get chapter names in\n",
    "\n",
    "for x in range(len(list_of_books)):\n",
    "    book = list_of_books[x]\n",
    "    segs = list_of_segments[x]\n",
    "    \n",
    "    for i in range(len(book)):\n",
    "        if x == 0:\n",
    "            temp1 = []\n",
    "            temp2 = 0.0\n",
    "            temp1,temp2 = Polarity(segs[i])\n",
    "            silm_polarity_lists.append(temp1)\n",
    "            silm_polarity_avg.append(temp2)\n",
    "        if x == 1:\n",
    "            temp1 = []\n",
    "            temp2 = 0.0\n",
    "            temp1,temp2 = Polarity(segs[i])\n",
    "            hobbit_polarity_lists.append(temp1)\n",
    "            hobbit_polarity_avg.append(temp2)\n",
    "        if x == 2:\n",
    "            temp1 = []\n",
    "            temp2 = 0.0\n",
    "            temp1,temp2 = Polarity(segs[i])\n",
    "            fellowship_polarity_lists.append(temp1)\n",
    "            fellowship_polarity_avg.append(temp2)\n",
    "        if x == 3:\n",
    "            temp1 = []\n",
    "            temp2 = 0.0\n",
    "            temp1,temp2 = Polarity(segs[i])\n",
    "            twotowers_polarity_lists.append(temp1)\n",
    "            twotowers_polarity_avg.append(temp2)\n",
    "        if x == 4:\n",
    "            temp1 = []\n",
    "            temp2 = 0.0\n",
    "            temp1,temp2 = Polarity(segs[i])\n",
    "            return_polarity_lists.append(temp1)\n",
    "            return_polarity_avg.append(temp2)\n",
    "\n",
    "all_polarity_avgs = [silm_polarity_avg, hobbit_polarity_avg, fellowship_polarity_avg, twotowers_polarity_avg, return_polarity_avg]\n",
    "\n",
    "all_polarity_lists = [silm_polarity_lists, hobbit_polarity_lists, fellowship_polarity_lists, twotowers_polarity_lists, return_polarity_lists]  \n",
    "# chapter 3 of return of the king is super dark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Commented this all out so I don't reset my excel work every time\n",
    "\"\"\"\n",
    "# Converting Polarity AVG data into pandas dataframes\n",
    "## These CSVs store all average chapter polarities for each book\n",
    "silm_df = pd.DataFrame(silm_polarity_avg, index = silm_chapter_names, columns = [\"Polarity\"])\n",
    "silm_df = silm_df.rename_axis(\"--- The Silmarillion ---\")\n",
    "#silm_df.to_csv(\"silm_df.csv\")\n",
    "\n",
    "hobbit_df = pd.DataFrame(hobbit_polarity_avg, index = hobbit_chapter_names, columns = [\"Polarity\"])\n",
    "hobbit_df = hobbit_df.rename_axis(\"--- The Hobbit ---\")\n",
    "#hobbit_df.to_csv(\"hobbit_df.csv\")\n",
    "\n",
    "fellowship_df = pd.DataFrame(fellowship_polarity_avg, index = fellowship_chapter_names, columns = [\"Polarity\"])\n",
    "fellowship_df = fellowship_df.rename_axis(\"--- The Fellowship of the Ring ---\")\n",
    "# Prologue chapters have weird polarities - have solid values because they're shorter\n",
    "#fellowship_df.to_csv(\"fellowship_df.csv\")\n",
    "\n",
    "twotowers_df = pd.DataFrame(twotowers_polarity_avg, index = twotowers_chapter_names, columns = [\"Polarity\"])\n",
    "twotowers_df = twotowers_df.rename_axis(\"--- The Two Towers ---\")\n",
    "#twotowers_df.to_csv(\"twotowers_df.csv\")\n",
    "\n",
    "return_df = pd.DataFrame(return_polarity_avg, index = return_chapter_names, columns = [\"Polarity\"])\n",
    "return_df = return_df.rename_axis(\"--- The Return of the King ---\")\n",
    "#return_df.to_csv(\"return_df.csv\")\n",
    "\n",
    "# Dataframe of all Books overlaid\n",
    "books_df = [silm_df, hobbit_df, fellowship_df, twotowers_df, return_df]\n",
    "full_df = pd.concat(books_df)\n",
    "\n",
    "# Export to CSV\n",
    "full_df.to_csv(\"full_df.csv\")\n",
    "\n",
    "\n",
    "# Exported to Excel as well, for simpler plots\n",
    "excel = pd.ExcelWriter('LOTR1.xlsx')\n",
    "silm_df.to_excel(excel, 'The Silmarillion')\n",
    "hobbit_df.to_excel(excel, 'The Hobbit')\n",
    "fellowship_df.to_excel(excel, 'Fellowship of the Rings')\n",
    "twotowers_df.to_excel(excel, 'The Two Towers')\n",
    "return_df.to_excel(excel, 'The Return of the King')\n",
    "full_df.to_excel(excel, 'Combined')\n",
    "excel.save()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, commented out data exports\n",
    "\"\"\"\n",
    "# Converting Polarity List data into pandas dataframes\n",
    "## These CSVs store all polarity ratings for each book, rather than average chapter polarity ratings\n",
    "### Tried to make the ratings graphically readable by converting them to -1,0,1, but there's just no shot\n",
    "silm_all_pol = []\n",
    "for i in range(len(silm_polarity_lists)):\n",
    "    for polarity in silm_polarity_lists[i]:\n",
    "        if polarity == 0: \n",
    "            polarity = -1\n",
    "        elif polarity == 2:\n",
    "            polarity = 0\n",
    "        else:\n",
    "            polarity = 1\n",
    "        silm_all_pol.append(polarity)\n",
    "silm_all_pol = pd.DataFrame(silm_all_pol, columns = [\"Polarity\"])\n",
    "\n",
    "hobbit_all_pol = []\n",
    "for i in range(len(hobbit_polarity_lists)):\n",
    "    for polarity in hobbit_polarity_lists[i]:\n",
    "        if polarity == 0:\n",
    "            polarity = -1\n",
    "        elif polarity == 2:\n",
    "            polarity = 0\n",
    "        else:\n",
    "            polarity = 1\n",
    "        hobbit_all_pol.append(polarity)\n",
    "hobbit_all_pol = pd.DataFrame(hobbit_all_pol, columns = [\"Polarity\"])\n",
    "\n",
    "fellowship_all_pol = []\n",
    "for i in range(len(fellowship_polarity_lists)):\n",
    "    for polarity in fellowship_polarity_lists[i]:\n",
    "        if polarity == 0:\n",
    "            polarity = -1\n",
    "        elif polarity == 2:\n",
    "            polarity = 0\n",
    "        else:\n",
    "            polarity = 1\n",
    "        fellowship_all_pol.append(polarity)\n",
    "fellowship_all_pol = pd.DataFrame(fellowship_all_pol, columns = [\"Polarity\"])\n",
    "\n",
    "twotowers_all_pol = []\n",
    "for i in range(len(twotowers_polarity_lists)):\n",
    "    for polarity in twotowers_polarity_lists[i]:\n",
    "        if polarity == 0:\n",
    "            polarity = -1\n",
    "        elif polarity == 2:\n",
    "            polarity = 0\n",
    "        else:\n",
    "            polarity = 1\n",
    "        twotowers_all_pol.append(polarity)\n",
    "twotowers_all_pol = pd.DataFrame(twotowers_all_pol, columns = [\"Polarity\"])\n",
    "\n",
    "return_all_pol = []\n",
    "for i in range(len(return_polarity_lists)):\n",
    "    for polarity in return_polarity_lists[i]:\n",
    "        if polarity == 0:\n",
    "            polarity = -1\n",
    "        elif polarity == 2:\n",
    "            polarity = 0\n",
    "        else:\n",
    "            polarity = 1\n",
    "        return_all_pol.append(polarity)\n",
    "return_all_pol = pd.DataFrame(return_all_pol, columns = [\"Polarity\"])\n",
    "\n",
    "all_pol_list = [silm_all_pol, hobbit_all_pol, fellowship_all_pol, twotowers_all_pol, return_all_pol]\n",
    "all_pol = pd.concat(all_pol_list)\n",
    "\n",
    "excel2 = pd.ExcelWriter('LOTR2.xlsx')\n",
    "silm_all_pol.to_excel(excel2, 'The Silmarillion')\n",
    "hobbit_all_pol.to_excel(excel2, 'The Hobbit')\n",
    "fellowship_all_pol.to_excel(excel2, 'Fellowship of the Rings')\n",
    "twotowers_all_pol.to_excel(excel2, 'The Two Towers')\n",
    "return_all_pol.to_excel(excel2, 'The Return of the King')\n",
    "all_pol.to_excel(excel2, 'Combined')\n",
    "\n",
    "excel2.save()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 893,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function that tokenizes and cleans the words of every chapter in a book.\n",
    "\n",
    "Parameters - book - a book.\n",
    "\n",
    "Returns a List of Lists storing a tokenized list for every chapter in a book.\n",
    "\"\"\"\n",
    "\n",
    "def Tokenize(book):\n",
    "    punctuation = \".,;!?:`'()’■''\" ## including strange symbols\n",
    "    token_list = []\n",
    "    for chapter in book:\n",
    "        temp = []\n",
    "        words = nltk.word_tokenize(chapter)\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            if word not in punctuation and not word.isnumeric(): \n",
    "                temp.append(word)\n",
    "        token_list.append(temp)\n",
    "        \n",
    "    return token_list\n",
    "\n",
    "\n",
    "#tokens = (Tokenize(silm_chapters))\n",
    "#print(tokens[0]) ## Tokens of first chapter of The Silmarillion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 900,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function that returns the n most common words for every chapter in a book.\n",
    "This is accomplished by using 'Counter' in the 'Collections' module.\n",
    "\n",
    "Parameters - book - a tokenized list of lists of all chapters of a book\n",
    "          - n - number of most common words in the chapter\n",
    "          \n",
    "Returns a List of Lists of the n most common words of every chapter in the book.\n",
    "\"\"\"\n",
    "\n",
    "def MostCommon(book, n): \n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    \n",
    "    names = [\"gandalf\", \"merry\", \"pippin\", \"frodo\", \"sam\", \"aragorn\", \"faramir\", \"denethor\", \"gimli\", \"legolas\"] # list of common character names\n",
    "    \n",
    "    tolkien_stop = [\"men\",\"great\", \"'s\", \"said\", \"went\", \"he\", \"would\", \"many\", \"one\", \"he\", \"came\", \"yet\", \"even\", \"shall\", \\\n",
    "                   \"upon\", \"days\", \"looked\", \"n't\", \"back\", \"could\", \"'ll\", \"'ve\", \"come\", \"still\", \"gate\", \"'i\", \"yield\" ]\n",
    "    \n",
    "    # Have to get rid of a lot of words, I call these \"tolkien stop words\", the silmarillion is full of these,\n",
    "    # in LOTR, 'great' and 'men' appear very often\n",
    "    \n",
    "    common_words = []\n",
    "    for chapter in book:\n",
    "        temp = []\n",
    "        for word in chapter:\n",
    "            if word not in stop_words and word not in names and word not in tolkien_stop:\n",
    "                temp.append(word)\n",
    "                  \n",
    "        common_words.append(Counter(temp).most_common(n))\n",
    "    return common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Find top 5 NON-STOP words per chapter\n",
    "\n",
    "silm_chapters_tokenized = []\n",
    "hobbit_chapters_tokenized = []\n",
    "fellowship_chapters_tokenized = []\n",
    "twotowers_chapters_tokenized = []\n",
    "return_chapters_tokenized = []\n",
    "\n",
    "silm_chapters_common = []\n",
    "hobbit_chapters_common = []\n",
    "fellowship_chapters_common = []\n",
    "twotowers_chapters_common = []\n",
    "return_chapters_common = []\n",
    "\n",
    "for i in range(len(list_of_books)):\n",
    "    if i == 0:\n",
    "        silm_chapters_tokenized = Tokenize(list_of_books[i])\n",
    "        silm_chapters_common = MostCommon(silm_chapters_tokenized, 20)\n",
    "    if i == 1:\n",
    "        hobbit_chapters_tokenized = Tokenize(list_of_books[i])\n",
    "        hobbit_chapters_common = MostCommon(hobbit_chapters_tokenized, 20)\n",
    "    if i == 2:\n",
    "        fellowship_chapters_tokenized = Tokenize(list_of_books[i])\n",
    "        fellowship_chapters_common = MostCommon(fellowship_chapters_tokenized, 20)\n",
    "    if i == 3:\n",
    "        twotowers_chapters_tokenized = Tokenize(list_of_books[i])\n",
    "        twotowers_chapters_common = MostCommon(twotowers_chapters_tokenized, 20)\n",
    "    if i == 4:\n",
    "        return_chapters_tokenized = Tokenize(list_of_books[i])\n",
    "        return_chapters_common = MostCommon(return_chapters_tokenized, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA - Latent Dirichlet Allocation \n",
    "- tokenize\n",
    "- remove stop words\n",
    "- lemmatize tokens\n",
    "- vectorize\n",
    "- model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function that lemmatizes a word.\n",
    "\n",
    "Parameters - word - word to be lemmatized\n",
    "\n",
    "Returns the lemmatized version of the word.\n",
    "\"\"\"\n",
    "def lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 912,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function prepares tokens to be sent through the LDA process. In doing so, it needs to make sure the tokens are\n",
    "words, they aren't in the 'Tolkien Stop Words' and that they are lemmas.\n",
    "The Latent Dirichlet Allocation function will remove stop words and ignore punctuation.\n",
    "\n",
    "Parameters - tokens - tokens of a chapter to lematize.\n",
    "\n",
    "Returns a list of tokens that are ready to be joined back together for Latent Dirichlet Allocation.\n",
    "\"\"\"\n",
    "def LDA_prepare(tokens):\n",
    "    ret = []\n",
    "    for word in tokens:\n",
    "        if word.isalpha() and word not in tolkien_stop:\n",
    "            word = word.lower()\n",
    "            ret.append(word)\n",
    "            \n",
    "    for word in ret:\n",
    "        word = lemma(word)\n",
    "        \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative Chapter Names:    \n",
    "</br>\n",
    "    - A Knife in the Dark    \n",
    "    - The Departure of Boromir    \n",
    "    - The Ride of Rohirrim     \n",
    "    - The Battle of the Pelennor Fields    \n",
    "    - The Pyre of Denethor    \n",
    "    - The Black Gate Opens    \n",
    "    - Mount Doom   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 954,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 194)\t1\n",
      "  (2, 721)\t1\n",
      "  (5, 297)\t1\n",
      "  (8, 986)\t1\n",
      "  (10, 1193)\t1\n",
      "  (13, 696)\t1\n",
      "  (15, 145)\t1\n",
      "  (16, 299)\t1\n",
      "  (17, 742)\t1\n",
      "  (19, 162)\t1\n",
      "  (21, 844)\t1\n",
      "  (22, 1297)\t1\n",
      "  (25, 323)\t1\n",
      "  (29, 1067)\t1\n",
      "  (31, 672)\t1\n",
      "  (33, 265)\t1\n",
      "  (34, 1283)\t1\n",
      "  (35, 1174)\t1\n",
      "  (36, 458)\t1\n",
      "  (37, 122)\t1\n",
      "  (38, 911)\t1\n",
      "  (40, 358)\t1\n",
      "  (41, 185)\t1\n",
      "  (43, 945)\t1\n",
      "  (46, 465)\t1\n",
      "  :\t:\n",
      "  (8635, 752)\t1\n",
      "  (8636, 1155)\t1\n",
      "  (8639, 1335)\t1\n",
      "  (8640, 184)\t1\n",
      "  (8644, 1334)\t1\n",
      "  (8645, 844)\t1\n",
      "  (8647, 563)\t1\n",
      "  (8649, 1303)\t1\n",
      "  (8650, 745)\t1\n",
      "  (8654, 299)\t1\n",
      "  (8657, 485)\t1\n",
      "  (8658, 137)\t1\n",
      "  (8660, 1532)\t1\n",
      "  (8663, 600)\t1\n",
      "  (8667, 402)\t1\n",
      "  (8668, 535)\t1\n",
      "  (8669, 379)\t1\n",
      "  (8671, 1336)\t1\n",
      "  (8672, 1197)\t1\n",
      "  (8674, 1062)\t1\n",
      "  (8677, 480)\t1\n",
      "  (8679, 217)\t1\n",
      "  (8681, 1060)\t1\n",
      "  (8682, 600)\t1\n",
      "  (8683, 1372)\t1\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and prepare all negative chapters for Latent Dirichlet Allocation\n",
    "## Then Vectorize all tokenized chapters\n",
    "\n",
    "## Vectorizer removes words that appear more than 15%, stop words, the 'analyzer' argument\n",
    "## allows us to fit with a list of tokens\n",
    "negative_chapter_tokens = []\n",
    "negative_chapter_vectors = []\n",
    "negative_chapter_joins = []\n",
    "\n",
    "# Initialize Vectorizer\n",
    "vect = CountVectorizer(max_features=10000, max_df=.15, stop_words='english', analyzer = 'word')\n",
    "\n",
    "\"\"\"\n",
    "# Tokenize\n",
    "for chapter in negative_chapters:\n",
    "    tokens = nltk.word_tokenize(chapter)\n",
    "    negative_chapter_tokens.append(LDA_prepare(tokens))\n",
    "\"\"\"\n",
    "#print(negative_chapters[0])\n",
    "\n",
    "tempy = nltk.word_tokenize(negative_chapters[0])\n",
    "tempy = LDA_prepare(tempy)\n",
    "\n",
    "neg = [''.join(filter(str.isalpha, word)) for word in tempy]\n",
    "\n",
    "temp = vect.fit_transform(neg) # Feed tokens into vectorizer and append to Vector list\n",
    "\n",
    "print(temp)\n",
    "#print(neg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 956,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lda.components_.shape: (10, 1408)\n",
      "topic 0       topic 1       topic 2       topic 3       topic 4       \n",
      "--------      --------      --------      --------      --------      \n",
      "master        frodo         mountain      black         eyes          \n",
      "far           gollum        end           thing         did           \n",
      "hand          light         fell          path          strength      \n",
      "deep          road          high          mr            felt          \n",
      "north         shadow        knew          time          passed        \n",
      "ago           water         voice         west          rose          \n",
      "need          head          know          grew          thought       \n",
      "breast        left          plain         ca            heard         \n",
      "began         stone         sword         rising        vast          \n",
      "took          yes           clouds        half          spoke         \n",
      "\n",
      "\n",
      "topic 5       topic 6       topic 7       topic 8       topic 9       \n",
      "--------      --------      --------      --------      --------      \n",
      "dark          sam           stood         land          away          \n",
      "heart         like          hands         ring          day           \n",
      "mind          turned        slowly        things        way           \n",
      "ground        long          grey          night         saw           \n",
      "going         doom          longer        drew          fear          \n",
      "just          little        cried         moment        gave          \n",
      "words         precious      gone          carry         suddenly      \n",
      "red           lay           bit           tower         dreadful      \n",
      "sauron        feet          muttered      wild          die           \n",
      "weight        knees         power         new           burden        \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Topic modeling on certain chapters of LOTR\n",
    "path= '/Users/blaz/Desktop/LOTR/return-chapters'\n",
    "\n",
    "for file in sorted(glob.glob(os.path.join(path,'*.txt'))):\n",
    "    if file == \"/Users/blaz/Desktop/LOTR/return-chapters/13.txt\":\n",
    "        break\n",
    "        \n",
    "MountDoomFile = open(\"/Users/blaz/Desktop/LOTR/return-chapters/13.txt\", 'r')\n",
    "MountDoomFile = MountDoomFile.read()\n",
    "MDF = nltk.word_tokenize(MountDoomFile)\n",
    "MDF_words = []\n",
    "\n",
    "\n",
    "### Remove Tolkien Stop words and then join the chapter back together for the Count Vectorizer\n",
    "for word in MDF:\n",
    "    if word not in tolkien_stop:\n",
    "        MDF_words.append(word)\n",
    "        \n",
    "MD_join = [' '.join(filter(str.isalpha, word.lower().split())) for word in MDF_words]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vect = CountVectorizer(max_features=10000, max_df=.15, stop_words='english', analyzer = 'word')\n",
    "MountDoom = vect.fit_transform(MDF_words)\n",
    "\n",
    "\n",
    "### 10 topics (components)\n",
    "lda = LatentDirichletAllocation(n_components=10, learning_method=\"batch\",\n",
    "                                max_iter=30, random_state=0)\n",
    "\n",
    "document_topics = lda.fit_transform(MountDoom)\n",
    "\n",
    "\n",
    "print(\"lda.components_.shape: {}\".format(lda.components_.shape))\n",
    "lda.components_.shape: (10, 10000)\n",
    "\n",
    "sorting = np.argsort(lda.components_, axis=1)[:, ::-1]\n",
    "\n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "\n",
    "mglearn.tools.print_topics(topics=range(10), feature_names=feature_names, \n",
    "                           sorting=sorting, topics_per_chunk=5, n_words=10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 957,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0       topic 1       topic 2       topic 3       topic 4       \n",
      "--------      --------      --------      --------      --------      \n",
      "need          yield         yield         child         yield         \n",
      "ragged        yess          yess          hobbit        yess          \n",
      "yield         foul          foul          yield         foul          \n",
      "frodo         foundations   foundations   foul          foundations   \n",
      "foul          founded       founded       founded       founded       \n",
      "foundations   fount         fount         fount         fount         \n",
      "founded       fours         fours         fours         fours         \n",
      "fount         fox           fox           fox           fox           \n",
      "fours         free          free          free          free          \n",
      "fox           fro           fro           fro           fro           \n",
      "\n",
      "\n",
      "topic 5       topic 6       topic 7       topic 8       topic 9       \n",
      "--------      --------      --------      --------      --------      \n",
      "yield         yield         step          fair          yield         \n",
      "yess          yess          yield         yield         yess          \n",
      "foul          foul          fost          gaping        foul          \n",
      "foundations   foundations   foul          foundations   foundations   \n",
      "founded       founded       foundations   founded       founded       \n",
      "fount         fount         founded       fount         fount         \n",
      "fours         fours         fount         fours         fours         \n",
      "fox           fox           fours         fox           fox           \n",
      "free          free          fox           free          free          \n",
      "fro           fro           free          fro           fro           \n",
      "\n",
      "\n",
      "topic 10      topic 11      topic 12      topic 13      topic 14      \n",
      "--------      --------      --------      --------      --------      \n",
      "yield         yield         yield         drab          earth         \n",
      "yess          yess          yess          hued          sheer         \n",
      "foul          foul          foul          fought        amid          \n",
      "foundations   foundations   foundations   foundations   sided         \n",
      "founded       founded       founded       founded       middle        \n",
      "fount         fount         fount         fount         yield         \n",
      "fours         fours         fours         fours         foundations   \n",
      "fox           fox           fox           fox           founded       \n",
      "free          free          free          free          fount         \n",
      "fro           fro           fro           fro           fours         \n",
      "\n",
      "\n",
      "topic 15      topic 16      topic 17      topic 18      topic 19      \n",
      "--------      --------      --------      --------      --------      \n",
      "road          yield         yield         yield         yield         \n",
      "ore           yess          yess          yess          yess          \n",
      "shield        foul          foul          foul          foul          \n",
      "fought        foundations   foundations   foundations   foundations   \n",
      "foundations   founded       founded       founded       founded       \n",
      "founded       fount         fount         fount         fount         \n",
      "fount         fours         fours         fours         fours         \n",
      "fours         fox           fox           fox           fox           \n",
      "fox           free          free          free          free          \n",
      "free          fro           fro           fro           fro           \n",
      "\n",
      "\n",
      "topic 20      topic 21      topic 22      topic 23      topic 24      \n",
      "--------      --------      --------      --------      --------      \n",
      "north         yield         ores          yield         yield         \n",
      "rope          yess          yield         yess          yess          \n",
      "east          foul          grey          foul          foul          \n",
      "elven         foundations   foul          foundations   foundations   \n",
      "frodo         founded       foundations   founded       founded       \n",
      "foul          fount         founded       fount         fount         \n",
      "foundations   fours         fount         fours         fours         \n",
      "founded       fox           fours         fox           fox           \n",
      "fount         free          fox           free          free          \n",
      "fours         fro           free          fro           fro           \n",
      "\n",
      "\n",
      "topic 25      topic 26      topic 27      topic 28      \n",
      "--------      --------      --------      --------      \n",
      "yield         wide          stones        yield         \n",
      "yess          splayed       yield         yess          \n",
      "foul          yield         frodo         foul          \n",
      "foundations   fume          foul          foundations   \n",
      "founded       foundations   foundations   founded       \n",
      "fount         founded       founded       fount         \n",
      "fours         fount         fount         fours         \n",
      "fox           fours         fours         fox           \n",
      "free          fox           fox           free          \n",
      "fro           free          free          fro           \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### 100 topics (components)\n",
    "lda100 = LatentDirichletAllocation(n_components=100, learning_method=\"batch\",\n",
    "                                   max_iter=30, random_state=0)\n",
    "\n",
    "document_topics100 = lda100.fit_transform(MountDoom)\n",
    "\n",
    "topics = np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28])\n",
    "\n",
    "sorting = np.argsort(lda100.components_, axis=1)[:, ::-1]\n",
    "\n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "mglearn.tools.print_topics(topics=topics, feature_names=feature_names,\n",
    "                           sorting=sorting, topics_per_chunk=5, n_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Calculate all frequencies\n",
    "\n",
    "silm_freq = {}\n",
    "hobbit_freq = {}\n",
    "fellowship_freq = {}\n",
    "twotowers_freq = {}\n",
    "return_freq = {}\n",
    "\n",
    "freq_dicts = [silm_freq, hobbit_freq, fellowship_freq, twotowers_freq, return_freq]\n",
    "\n",
    "for i in range(len(tokenlist)):\n",
    "    for word in tokenlist[i]:\n",
    "        word = word.lower()\n",
    "        if word not in punctuation and not word.isnumeric() and word.isalpha():\n",
    "            if word in freq_dicts[i]:\n",
    "                freq_dicts[i][word] += 1\n",
    "            else:\n",
    "                freq_dicts[i][word] = 1\n",
    "\n",
    "entirety_dict = {}\n",
    "\n",
    "for word in entirety:\n",
    "    word = word.lower()\n",
    "    if word not in punctuation and not word.isnumeric() and word.isalpha():\n",
    "        if word in entirety_dict:\n",
    "            entirety_dict[word] += 1\n",
    "        else:\n",
    "            entirety_dict[word] = 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 899,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "The Silmarillion: 9 Out of 148,914 Words\n",
      "------------------------------------------------------\n",
      "The Hobbit: 1 Out of 96,180 Words\n",
      "------------------------------------------------------\n",
      "The Fellowship of the Ring: 4 Out of 182,858 Words\n",
      "------------------------------------------------------\n",
      "The Two Towers: 2 Out of 155,947 Words\n",
      "------------------------------------------------------\n",
      "The Return of the King: 4 Out of 132,059 Words\n",
      "------------------------------------------------------\n",
      "Entire Corpus: 20\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Function that returns the frequency of a given word in each book:\n",
    "\n",
    "Parameters - word - input word\n",
    "\n",
    "Returns the frequency of the input word in each book.\n",
    "\"\"\"\n",
    "\n",
    "#NOTE:: For whatever reason, the words \"orc\" and \"orcs\" were converted to \"ore\" and \"ores\"\n",
    "## I suppose this is the drawback of pulling 5 books off the internet\n",
    "\n",
    "\n",
    "def WordSearch(word):\n",
    "    word = word.lower() \n",
    "    for i in range(len(tokenlist)):\n",
    "        if i == 0:\n",
    "            if word in freq_dicts[i]:\n",
    "                print(\"------------------------------------------------------\")\n",
    "                print(\"The Silmarillion:\", freq_dicts[i][word], \"Out of 148,914 Words\")\n",
    "                print(\"------------------------------------------------------\")\n",
    "            else:\n",
    "                print(\"------------------------------------------------------\")\n",
    "                print(\"The Silmarillion: 0 Out of 148,914 Words\")   \n",
    "                print(\"------------------------------------------------------\")\n",
    "        if i == 1:\n",
    "            if word in freq_dicts[i]:\n",
    "                print(\"The Hobbit:\", freq_dicts[i][word], \"Out of 96,180 Words\")\n",
    "                print(\"------------------------------------------------------\")\n",
    "            else:\n",
    "                print(\"The Hobbit: 0 Out of 96,180 Words\")\n",
    "                print(\"------------------------------------------------------\")\n",
    "        if i == 2:\n",
    "            if word in freq_dicts[i]:\n",
    "                print(\"The Fellowship of the Ring:\", freq_dicts[i][word], \"Out of 182,858 Words\")\n",
    "                print(\"------------------------------------------------------\")\n",
    "            else:\n",
    "                print(\"The Fellowship of the Ring: 0 Out of 182,858 Words\")\n",
    "                print(\"------------------------------------------------------\")\n",
    "        if i == 3:\n",
    "            if word in freq_dicts[i]:\n",
    "                print(\"The Two Towers:\", freq_dicts[i][word], \"Out of 155,947 Words\")\n",
    "                print(\"------------------------------------------------------\")\n",
    "            else:\n",
    "                print(\"The Two Towers: 0 Out of 155,947 Words\")\n",
    "                print(\"------------------------------------------------------\")\n",
    "        if i == 4:\n",
    "            if word in freq_dicts[i]:\n",
    "                print(\"The Return of the King:\", freq_dicts[i][word], \"Out of 132,059 Words\")\n",
    "                print(\"------------------------------------------------------\")\n",
    "            else:\n",
    "                print(\"The Return of the King: 0 Out of 132,059 Words\")\n",
    "                print(\"------------------------------------------------------\")\n",
    "    if word in entirety_dict:\n",
    "        print(\"Entire Corpus:\", entirety_dict[word])\n",
    "        print(\"------------------------------------------------------\")\n",
    "                \n",
    "WordSearch(\"yield\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Lists of words to go to word clouds\n",
    "silm_cloud = []\n",
    "hobbit_cloud = []\n",
    "fellowship_cloud = []\n",
    "twotowers_cloud = []\n",
    "return_cloud = []\n",
    "\n",
    "cloudlist = [silm_cloud, hobbit_cloud, fellowship_cloud, twotowers_cloud, return_cloud]\n",
    "\n",
    "for i in range(len(tokenlist)):\n",
    "    for word in tokenlist[i]:\n",
    "        word=word.lower()\n",
    "        if word not in punctuation and word not in stop_words and not word.isnumeric() and word.isalpha():\n",
    "            if freq_dicts[i][word] > 5: ## Only words that occur 5 times or more\n",
    "                cloudlist[i].append(word)\n",
    "\n",
    "## Dataframes to send to Tableau\n",
    "\n",
    "silm_cloud_df = pd.DataFrame(cloudlist[0], columns = [\"The Silmarillion\"])\n",
    "hobbit_cloud_df = pd.DataFrame(cloudlist[1], columns = [\"The Hobbit\"])\n",
    "fellowship_cloud_df = pd.DataFrame(cloudlist[2], columns = [\"The Fellowship of the Ring\"])\n",
    "twotowers_cloud_df = pd.DataFrame(cloudlist[3], columns = [\"The Two Towers\"])\n",
    "return_cloud_df = pd.DataFrame(cloudlist[4], columns = [\"The Return of the King\"])\n",
    "\n",
    "\n",
    "cloud = pd.ExcelWriter('cloud.xlsx')\n",
    "\n",
    "silm_cloud_df.to_excel(cloud, 'The Silmarillion')\n",
    "hobbit_cloud_df.to_excel(cloud, 'The Hobbit')\n",
    "fellowship_cloud_df.to_excel(cloud, 'Fellowship of the Rings')\n",
    "twotowers_cloud_df.to_excel(cloud, 'The Two Towers')\n",
    "return_cloud_df.to_excel(cloud, 'The Return of the King')\n",
    "\n",
    "cloud.save()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Negative Word Cloud Exports\n",
    "\n",
    "negative_chapters = []\n",
    "negative_tokens = []\n",
    "negative_words = []\n",
    "\"\"\"\n",
    "A Knife in the Dark\n",
    "The Departure of Boromir\n",
    "The Ride of Rohirrim\n",
    "The Battle of the Pelennor Fields\n",
    "The Pyre of Denethor\n",
    "The Black Gate Opens\n",
    "Mount Doom\n",
    "\"\"\"\n",
    "negative_chapters.append(fellowship_chapters[14])\n",
    "negative_chapters.append(twotowers_chapters[0])\n",
    "negative_chapters.append(return_chapters[4])\n",
    "negative_chapters.append(return_chapters[5])\n",
    "negative_chapters.append(return_chapters[6])\n",
    "negative_chapters.append(return_chapters[9])\n",
    "negative_chapters.append(return_chapters[12])\n",
    "\n",
    "tolkien_stop = [\"men\",\"great\", \"'s\", \"said\", \"went\", \"he\", \"would\", \"many\", \"one\", \"he\", \"came\", \"yet\", \"even\", \"shall\", \\\n",
    "                \"upon\", \"days\", \"looked\", \"n't\", \"back\", \"could\", \"'ll\", \"'ve\", \"come\", \"still\", \"gate\", \"'i\" ]\n",
    "names = [\"gandalf\", \"merry\", \"pippin\", \"frodo\", \"sam\", \"aragorn\", \"faramir\", \"denethor\", \"gimli\", \"legolas\"]\n",
    "\n",
    "for chapter in negative_chapters:\n",
    "    negative_tokens.append(nltk.word_tokenize(chapter))\n",
    "  \n",
    "for i in range(len(negative_tokens)):\n",
    "    for word in negative_tokens[i]:\n",
    "        word = word.lower()\n",
    "        if word not in punctuation and word not in stop_words and not word.isnumeric()\\\n",
    "        and word.isalpha() and word not in tolkien_stop and word not in names:\n",
    "            negative_words.append(word)\n",
    "\n",
    "            \n",
    "neg_df = pd.DataFrame(negative_words, columns = [\"Negative Chapter Words\"])                 \n",
    "neg = pd.ExcelWriter('negative.xlsx')\n",
    "neg_df.to_excel(neg, 'Sheet 1')\n",
    "neg.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Positive Word Cloud Exports\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Working with 'glob'\n",
    "\n",
    "path= '/Users/blaz/Desktop/LOTR/silmarillion-chapters'\n",
    "silm_chapters = []\n",
    "\n",
    "for file in sorted(glob.glob(os.path.join(path,'*.txt'))):\n",
    "    print(file)\n",
    "    f = open(file, 'r')\n",
    "    txt = f.read()\n",
    "    silm_chapters.append(txt)\n",
    "    \n",
    "print(len(silm_chapters))\n",
    "print(silm_chapters[len(silm_chapters)-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
