{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Josh Blaz -- LOTR\n",
    "## CS401 -- NLP\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import urllib.request\n",
    "import lxml.html as lh\n",
    "import io\n",
    "import requests\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter \n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "#NOTE: Sentiment140 Polarity values: 0: negative, 2: neutral, 4: positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **NOTE: Elvish text is translated awkwardly into the .txt format    \n",
    "</br>\n",
    "##### IE:   \n",
    "</br>\n",
    "#### ►M MPR -F+MTRX MP ft PPtK P&RMPht: P. t. The last Two runes are the initials of Thror and Thrain.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Used Chapterize to split books into chapters \n",
    "## https://github.com/JonathanReeve/chapterize\n",
    "### Chapterize didn't work 100% perfectly, so I had to go through and the prologues back in when it cut them out\n",
    "\n",
    "# These are lists containing strings of every chapter for each book\n",
    "silm_chapters = []\n",
    "hobbit_chapters = []\n",
    "fellowship_chapters = []\n",
    "twotowers_chapters = []\n",
    "return_chapters = []\n",
    "\n",
    "# Paths to directories storing book chapters\n",
    "list_of_paths = ['/Users/blaz/Desktop/LOTR/silmarillion-chapters', '/Users/blaz/Desktop/LOTR/hobbit-chapters',\\\n",
    "                '/Users/blaz/Desktop/LOTR/fellowship-chapters', '/Users/blaz/Desktop/LOTR/twotowers-chapters',\\\n",
    "                '/Users/blaz/Desktop/LOTR/return-chapters']\n",
    "\n",
    "for path in list_of_paths: # iterate through the list of folder paths for each book\n",
    "    for file in sorted(glob.glob(os.path.join(path,'*.txt'))): # This gives us a sorted list of the files in each directory                                                         \n",
    "        f = open(file, 'r') # open and read file               # allowing us to read in the chapters in order.\n",
    "        txt = f.read()\n",
    "        ## determine which path we're using and append it to the correct book chapter list\n",
    "        if path == '/Users/blaz/Desktop/LOTR/silmarillion-chapters': \n",
    "            # Because of an issue with 'glob', I had to create a copy of the final chapter in The Silmarillion\n",
    "            silm_chapters.append(txt)\n",
    "        elif path == '/Users/blaz/Desktop/LOTR/hobbit-chapters':\n",
    "            hobbit_chapters.append(txt)\n",
    "        elif path == '/Users/blaz/Desktop/LOTR/fellowship-chapters': \n",
    "            fellowship_chapters.append(txt)\n",
    "        elif path == '/Users/blaz/Desktop/LOTR/twotowers-chapters': \n",
    "            twotowers_chapters.append(txt)\n",
    "        elif path == '/Users/blaz/Desktop/LOTR/return-chapters': \n",
    "            return_chapters.append(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store chapter names for use in dataframes later\n",
    "\n",
    "silm_chapter_names = [\"Ainundalë\", \"Valaquenta\", \"Of the Beginning of Days\", \"Of Aulë and Yavanna\" , \"Of the Coming of the Elves and the Captivity of Melkor\",\\\n",
    "                     \"Of Thingol and Melian\", \"Of Eldamar and the Princes of the Eldalië\", \"Of Fëanor and the Unchaining of Melkor\", \"Of the Silmarils and the Unrest of the Noldor\",\\\n",
    "                     \"Of the Darkening of Valinor\", \"Of the Flight of the Noldor\", \"Of the Sindar\", \"Of the Sun and Moon and the Hiding of Valinor\", \"Of Men\", \"Of the Return of the Noldor\",\\\n",
    "                     \"Of Beleriad and its Realms\", \"Of the Noldor in Beleriad\", \"Of Maeglin\", \"Of the Coming of Men into the West\", \"Of the Ruin of Beleriad and the Fall of Fingolfin\", \"Of Beren and Lúthien\",\\\n",
    "                     \"Of the Fifth Battle: Nirnaeth Arnoediad\", \"Of Túrin Turambar\", \"Of the Ruin of Doriath\", \"Of Tuor and the Fall of Gondolin\", \"Of the Voyage of Eärendil and the War of Wrath\", \\\n",
    "                     \"Akallabêth: The Downfall of Númenor\", \"Of the Rings of Power and the Third Age\"]\n",
    "\n",
    "hobbit_chapter_names = [\"An Unexpected Party\", \"Roast Mutton\", \"A Short Rest\", \"Over Hill and Under Hill\", \"Riddles In The Dark\", \\\n",
    "                       \"Out Of The Frying-Pan Into The Fire\", \"Queer Lodgings\", \"Flies And Spiders\", \"Barrels Out Of Bond\", \"A Warm Welcome\", \\\n",
    "                       \"On The Doorstep\", \"Inside Information\", \"Not At Home\", \"Fire And Water\", \"The Gathering Of The Clouds\", \"A Thief In The Night\", \\\n",
    "                       \"The Clouds Burst\", \"The Return Journey\", \"The Last Stage\"]\n",
    "\n",
    "fellowship_chapter_names = [\"Concerning Hobbits\", \"Concerning Pipeweed\", \"Of the Ordering of the Shire\", \"Note on the Shire Records\", \"A Long-expected Party\", \"The Shadow of the Past\", \\\n",
    "                           \"Three is Company\", \"A Short Cut to Mushrooms\", \"A Conspiracy Unmasked\", \"The Old Forest\", \"In the House of Tom Bombadil\", \"Fog on the Barrow-downs\", \"At the Sign of the Prancing Pony\",\\\n",
    "                           \"Strider\", \"A Knife in the Dark\", \"Flight to the Ford\", \"Many Meetings\", \"The Council of Elrond\", \"The Ring goes South\", \"A Journey in the Dark\", \"The Bridge of Khazad-dûm\", \\\n",
    "                           \"Lothlórien\", \"The Mirror of Galadriel\", \"Farewell to Lórien\", \"The Great River\", \"The Breaking of the Fellowship\"]\n",
    "\n",
    "twotowers_chapter_names = [\"The Departure of Boromir\", \"The Riders of Rohan\", \"The Uruk-hai\", \"Treebeard\", \"The White Rider\", \"The King of the Golden Hall\", \"Helm's Deep\", \"The Road to Isengard\", \"Flotsam and Jetsam\", \\\n",
    "                          \"The Voice of Saruman\", \"The Palantír\", \"The Taming of Smeagol\", \"The Passage of the Marshes\", \"The Black Gate is Closed\", \"Of Herbs and Stewed Rabbit\", \"The Window of the West\", \"The Forbidden Pool\", \\\n",
    "                          \"Journey to the Cross-roads\", \"The Stairs to Cirith Ungol\", \"Shelob's Lair\", \"The Choices of Master Samwise\"]\n",
    "\n",
    "return_chapter_names = [\"Minas Tirith\", \"The Passing of the Grey Company\", \"The Muster of Rohan\", \"The Siege of Gondor\", \"The Ride of Rohirrim\", \"The Battle of the Pelennor Fields\", \"The Pyre of Denethor\",\\\n",
    "                       \"The Houses of Healing\", \"The Last Debate\", \"The Black Gate Opens\", \"The Tower of Cirith Ungol\", \"The Land of Shadow\", \"Mount Doom\", \"The Field of Cormallen\", \"The Steward and the King\", \\\n",
    "                       \"Many Partings\", \"Homeward Bound\", \"Scouring of the Shire\", \"The Grey Havens\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Steps**\n",
    "</br>\n",
    "### ** 1. Segment all chapters into page-sized objects    **    \n",
    "</br>\n",
    "### ** 2. Send all segments to Sentiment140 API by chapter    **   \n",
    "</br>\n",
    "### ** 3. Calculate polarity averages and polarity lists. **   \n",
    "</br>\n",
    "### ** 4. Store API polarity ratings and send export to csv to plot**    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function that segments given chapter into a page-sized (2940 character) segments to be sent to the API.\n",
    "\n",
    "Parameters - chapter - chapter of a book to be broken into segments\n",
    "           - cut - length that we segment the text with\n",
    "       \n",
    "Returns a list of (string) segments of the chapter.\n",
    "\"\"\"\n",
    "def Segmenter(chapter, cut):\n",
    "    segments = []\n",
    "    # start and end indices for segmenting the text\n",
    "    start = 0\n",
    "    end = cut\n",
    "    while end < len(chapter) + cut:\n",
    "        segments.append(chapter[start:end])\n",
    "        start = end\n",
    "        end = end + cut\n",
    "    return segments #segments of input chapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This function allows us to split the chapters of each book into segments to send in our HTTP-Post JSON requests.   \n",
    "</br>\n",
    "I chose 2940 as the length because this is the exact number of characters per page (including spaces) in my copy of Fellowship of the Ring. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lists of Lists of Lists storing all segments of all chapters for each book\n",
    "# [[chapter1 segment 0-2500, chap1, segmenet 2500-5000]... [chapter2 segment0-2500, ...]...]\n",
    "silm_segments = []\n",
    "hobbit_segments = []\n",
    "fellowship_segments = []\n",
    "twotowers_segments = []\n",
    "return_segments = []\n",
    "\n",
    "# List containing the lists storing each books' chapters\n",
    "list_of_books = [silm_chapters, hobbit_chapters, fellowship_chapters, twotowers_chapters, return_chapters]\n",
    "# List allowing us to access the segment lists\n",
    "list_of_segments = [silm_segments, hobbit_segments, fellowship_segments, twotowers_segments, return_segments]\n",
    "\n",
    "for i in range(len(list_of_books)):\n",
    "    for chapter in list_of_books[i]: # Segment entire chapter using Segmenter function, with 2940 character cuts\n",
    "        list_of_segments[i].append(Segmenter(chapter,2940))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the cell above I create lists of lists for segments of each chapter of each corpus or book, and append to them using my \"Segmenter\"\n",
    "function, storing them neatly like this will allow me to iteratively query the API server. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function that sends segments of 1 chapter through the Sentiment140 API.\n",
    "In order to do so, it creates and appends segments to a JSON file, then posts the JSON queries to the API server\n",
    "using requests module (using an HTTP Post)\n",
    "\n",
    "Parameters - chapter_segments - segments of an entire chapter of a book\n",
    "\n",
    "Returns a list of polarities for segments of the chapter, as well as the polarity average for the chapter\n",
    "\n",
    "Note: Maximum of 700,000 characters per API request, though this shouldn't be a problem\n",
    "\"\"\"\n",
    "\n",
    "def Polarity(chapter_segments): # segments of a single chapter\n",
    "    request = {'data':[]}\n",
    "    polarityList = []\n",
    "    counter = 0\n",
    "    for segment in chapter_segments: # Fill JSON\n",
    "        request['data'].append({'text':segment})\n",
    "    r = requests.post('http://www.sentiment140.com/api/bulkClassifyJson?appid=blaz_j1@denison.edu', json=request)\n",
    "    jso = r.json()\n",
    "    for i in range(len(request['data'])-1):\n",
    "        polarityList.append(jso['data'][i]['polarity'])\n",
    "    \n",
    "    polarityTotal = 0\n",
    "    for value in polarityList:\n",
    "        polarityTotal = polarityTotal + value\n",
    "    \n",
    "    polarityAVG = polarityTotal/len(polarityList)\n",
    "    #print(polarityList)\n",
    "    return polarityList, polarityAVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "silm\n",
      "hobbit\n",
      "fellowship\n",
      "two towers\n",
      "return of the king\n"
     ]
    }
   ],
   "source": [
    "# This function takes about a minute to run\n",
    "\n",
    "# store all averages, then store chapter avg, also overall average\n",
    "silm_polarity_avg = []\n",
    "hobbit_polarity_avg = []\n",
    "fellowship_polarity_avg = []\n",
    "twotowers_polarity_avg = []\n",
    "return_polarity_avg = []\n",
    "\n",
    "silm_polarity_lists = []\n",
    "hobbit_polarity_lists = []\n",
    "fellowship_polarity_lists = []\n",
    "twotowers_polarity_lists = []\n",
    "return_polarity_lists = []\n",
    "### Need to get chapter names in\n",
    "\n",
    "for x in range(len(list_of_books)):\n",
    "    book = list_of_books[x]\n",
    "    segs = list_of_segments[x]\n",
    "    \n",
    "    for i in range(len(book)):\n",
    "        if x == 0:\n",
    "            temp1 = []\n",
    "            temp2 = 0.0\n",
    "            temp1,temp2 = Polarity(segs[i])\n",
    "            silm_polarity_lists.append(temp1)\n",
    "            silm_polarity_avg.append(temp2)\n",
    "        if x == 1:\n",
    "            temp1 = []\n",
    "            temp2 = 0.0\n",
    "            temp1,temp2 = Polarity(segs[i])\n",
    "            hobbit_polarity_lists.append(temp1)\n",
    "            hobbit_polarity_avg.append(temp2)\n",
    "        if x == 2:\n",
    "            temp1 = []\n",
    "            temp2 = 0.0\n",
    "            temp1,temp2 = Polarity(segs[i])\n",
    "            fellowship_polarity_lists.append(temp1)\n",
    "            fellowship_polarity_avg.append(temp2)\n",
    "        if x == 3:\n",
    "            temp1 = []\n",
    "            temp2 = 0.0\n",
    "            temp1,temp2 = Polarity(segs[i])\n",
    "            twotowers_polarity_lists.append(temp1)\n",
    "            twotowers_polarity_avg.append(temp2)\n",
    "        if x == 4:\n",
    "            temp1 = []\n",
    "            temp2 = 0.0\n",
    "            temp1,temp2 = Polarity(segs[i])\n",
    "            return_polarity_lists.append(temp1)\n",
    "            return_polarity_avg.append(temp2)\n",
    "\n",
    "all_polarity_avgs = [silm_polarity_avg, hobbit_polarity_avg, fellowship_polarity_avg, twotowers_polarity_avg, return_polarity_avg]\n",
    "\n",
    "all_polarity_lists = [silm_polarity_lists, hobbit_polarity_lists, fellowship_polarity_lists, twotowers_polarity_lists, return_polarity_lists]  \n",
    "# chapter 3 of return of the king is super dark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Export to Excel\\npd.read_excel(\\'file.xlsx\\')\\nfull_df.to_csv(\"full_df.csv\")\\npd.to_excel(\\'dir/myDataFrame.xlsx\\', sheet_name=\\'Sheet1\\')\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting AVG data into pandas dataframes\n",
    "\n",
    "silm_df = pd.DataFrame(silm_polarity_avg, index = silm_chapter_names, columns = [\"Polarity\"])\n",
    "silm_df = silm_df.rename_axis(\"--- The Silmarillion ---\")\n",
    "#silm_df.to_csv(\"silm_df.csv\")\n",
    "\n",
    "hobbit_df = pd.DataFrame(hobbit_polarity_avg, index = hobbit_chapter_names, columns = [\"Polarity\"])\n",
    "hobbit_df = hobbit_df.rename_axis(\"--- The Hobbit ---\")\n",
    "#hobbit_df.to_csv(\"hobbit_df.csv\")\n",
    "\n",
    "fellowship_df = pd.DataFrame(fellowship_polarity_avg, index = fellowship_chapter_names, columns = [\"Polarity\"])\n",
    "fellowship_df = fellowship_df.rename_axis(\"--- The Fellowship of the Ring ---\")\n",
    "# Prologue chapters have weird polarities - have solid values because they're shorter\n",
    "#fellowship_df.to_csv(\"fellowship_df.csv\")\n",
    "\n",
    "twotowers_df = pd.DataFrame(twotowers_polarity_avg, index = twotowers_chapter_names, columns = [\"Polarity\"])\n",
    "twotowers_df = twotowers_df.rename_axis(\"--- The Two Towers ---\")\n",
    "#twotowers_df.to_csv(\"twotowers_df.csv\")\n",
    "\n",
    "return_df = pd.DataFrame(return_polarity_avg, index = return_chapter_names, columns = [\"Polarity\"])\n",
    "return_df = return_df.rename_axis(\"--- The Return of the King ---\")\n",
    "#return_df.to_csv(\"return_df.csv\")\n",
    "# Really dark novel\n",
    "\n",
    "# Dataframe of all Books overlaid\n",
    "books_df = [silm_df, hobbit_df, fellowship_df, twotowers_df, return_df]\n",
    "#full_df = pd.concat(books_df)\n",
    "\n",
    "# Export to CSV\n",
    "#full_df.to_csv(\"full_df.csv\")\n",
    "\n",
    "\"\"\"\n",
    "# Export to Excel\n",
    "pd.read_excel('file.xlsx')\n",
    "full_df.to_csv(\"full_df.csv\")\n",
    "pd.to_excel('dir/myDataFrame.xlsx', sheet_name='Sheet1')\n",
    "\"\"\"\n",
    "\n",
    "# Add another column for topic, once topic modeling is complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ainundalë                                                 0.0\n",
      "Valaquenta                                                4.0\n",
      "Of the Beginning of Days                                  4.0\n",
      "Of Aulë and Yavanna                                       0.0\n",
      "Of the Coming of the Elves and the Captivity of Melkor    4.0\n",
      "Of Thingol and Melian                                     NaN\n",
      "Of Eldamar and the Princes of the Eldalië                 4.0\n",
      "Of Fëanor and the Unchaining of Melkor                    0.0\n",
      "Of the Silmarils and the Unrest of the Noldor             0.0\n",
      "Of the Darkening of Valinor                               0.0\n",
      "Of the Flight of the Noldor                               0.0\n",
      "Of the Sindar                                             0.0\n",
      "Of the Sun and Moon and the Hiding of Valinor             4.0\n",
      "Of Men                                                    NaN\n",
      "Of the Return of the Noldor                               2.0\n",
      "Of Beleriad and its Realms                                4.0\n",
      "Of the Noldor in Beleriad                                 2.0\n",
      "Of Maeglin                                                0.0\n",
      "Of the Coming of Men into the West                        2.0\n",
      "Of the Ruin of Beleriad and the Fall of Fingolfin         0.0\n",
      "Of Beren and Lúthien                                      4.0\n",
      "Of the Fifth Battle: Nirnaeth Arnoediad                   2.0\n",
      "Of Túrin Turambar                                         0.0\n",
      "Of the Ruin of Doriath                                    0.0\n",
      "Of Tuor and the Fall of Gondolin                          0.0\n",
      "Of the Voyage of Eärendil and the War of Wrath            4.0\n",
      "Akallabêth: The Downfall of Númenor                       0.0\n",
      "Of the Rings of Power and the Third Age                   0.0\n",
      "Name: 2, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "silm_pl_df = pd.DataFrame(silm_polarity_lists, index = silm_chapter_names)\n",
    "#print(silm_pl_df)\n",
    "\n",
    "print(silm_pl_df[2])\n",
    "#silm_pl_df.to_csv(\"silm_pl_df_new.csv\")\n",
    "\n",
    "\n",
    "# Need help plotting this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function that tokenizes the words of every chapter in a book.\n",
    "\n",
    "Parameters - book - a book.\n",
    "\n",
    "Returns a List of Lists storing a tokenized list for every chapter in a book.\n",
    "\"\"\"\n",
    "\n",
    "def Tokenize(book):\n",
    "    punctuation = \".,;!?:`'()\"\n",
    "    token_list = []\n",
    "    for chapter in book:\n",
    "        temp = []\n",
    "        words = nltk.word_tokenize(chapter)\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            if word not in punctuation and not word.isnumeric(): # remove punctuation\n",
    "                temp.append(word)\n",
    "        token_list.append(temp)\n",
    "        \n",
    "    return token_list\n",
    "\n",
    "\n",
    "#tokens = (Tokenize(silm_chapters))\n",
    "#print(tokens[0]) ## Tokens of first chapter of The Silmarillion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function that returns the n most common words for every chapter in a book.\n",
    "This is accomplished by using 'Counter' in the 'Collections' module.\n",
    "\n",
    "Parameter - book - a tokenized list of lists of all chapters of a book\n",
    "          - n - number of most common words in the chapter\n",
    "          \n",
    "Returns a List of Lists of the n most common words of every chapter in the book.\n",
    "\"\"\"\n",
    "\n",
    "def MostCommon(book, n):\n",
    "    from collections import Counter \n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    \n",
    "    names = [\"gandalf\", \"merry\", \"pippin\", \"frodo\", \"sam\", \"aragorn\", \"faramir\", \"denethor\", \"gimli\", \"legolas\"] # list of common character names\n",
    "    \n",
    "    tolkien_stop = [\"men\",\"great\", \"'s\", \"said\", \"went\", \"he\", \"would\", \"many\", \"one\", \"he\", \"came\", \"yet\", \"even\", \"shall\", \\\n",
    "                   \"upon\", \"days\", \"looked\", \"n't\", \"back\", \"could\", \"'ll\", \"'ve\", \"come\", \"still\", \"gate\", \"'i\" ]\n",
    "    # Have to get rid of a lot of words, I call these \"tolkien stop words\", the silmarillion is full of these,\n",
    "    # in LOTR, 'great' and 'men' appear very often\n",
    "    \n",
    "    common_words = []\n",
    "    for chapter in book:\n",
    "        temp = []\n",
    "        for word in chapter:\n",
    "            if word not in stop_words and word not in names and word not in tolkien_stop:\n",
    "                temp.append(word)\n",
    "                  \n",
    "        common_words.append(Counter(temp).most_common(10))\n",
    "    return common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find top 5 NON-STOP words per chapter\n",
    "\n",
    "silm_chapters_tokenized = []\n",
    "hobbit_chapters_tokenized = []\n",
    "fellowship_chapters_tokenized = []\n",
    "twotowers_chapters_tokenized = []\n",
    "return_chapters_tokenized = []\n",
    "\n",
    "\n",
    "silm_chapters_common = []\n",
    "hobbit_chapters_common = []\n",
    "fellowship_chapters_common = []\n",
    "twotowers_chapters_common = []\n",
    "return_chapters_common = []\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(list_of_books)):\n",
    "    if i == 0:\n",
    "        silm_chapters_tokenized = Tokenize(list_of_books[i])\n",
    "        silm_chapters_common = MostCommon(silm_chapters_tokenized, 5)\n",
    "    if i == 1:\n",
    "        hobbit_chapters_tokenized = Tokenize(list_of_books[i])\n",
    "        hobbit_chapters_common = MostCommon(hobbit_chapters_tokenized, 5)\n",
    "    if i == 2:\n",
    "        fellowship_chapters_tokenized = Tokenize(list_of_books[i])\n",
    "        fellowship_chapters_common = MostCommon(fellowship_chapters_tokenized, 5)\n",
    "    if i == 3:\n",
    "        twotowers_chapters_tokenized = Tokenize(list_of_books[i])\n",
    "        twotowers_chapters_common = MostCommon(twotowers_chapters_tokenized, 5)\n",
    "    if i == 4:\n",
    "        return_chapters_tokenized = Tokenize(list_of_books[i])\n",
    "        return_chapters_common = MostCommon(return_chapters_tokenized, 5)\n",
    "\n",
    "\n",
    "# Try leaving tolkien words in and show most common words across entire series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA - Latent Dirichlet Allocation   \n",
    "</br>\n",
    "-- remove words that are %15 of the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import mglearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying out Topic Modeling on Return of the King first (entire text)\n",
    "\n",
    "ret = open('return.txt', 'r')\n",
    "\n",
    "vect = CountVectorizer(max_features=10000, max_df=.15)\n",
    "return_of_the_king = vect.fit_transform(ret)\n",
    "# Count Vectorizer removes all of the words that appear in at least 15% of the text\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=10, learning_method=\"batch\",\n",
    "                                max_iter=25, random_state=0)\n",
    "\n",
    "document_topics = lda.fit_transform(ret)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lda.components_.shape: (10, 1618)\n",
      "topic 0       topic 1       topic 2       topic 3       \n",
      "--------      --------      --------      --------      \n",
      "was           him           that          it            \n",
      "on            with          it            said          \n",
      "as            now           sam           with          \n",
      "at            frodo         had           last          \n",
      "all           at            for           its           \n",
      "frodo         it            could         them          \n",
      "with          in            on            sam           \n",
      "sam           for           in            at            \n",
      "for           but           as            my            \n",
      "now           be            go            down          \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Topic modeling on certain chapters of LOTR\n",
    "path= '/Users/blaz/Desktop/LOTR/return-chapters'\n",
    "\n",
    "for file in sorted(glob.glob(os.path.join(path,'*.txt'))):\n",
    "    if file == \"/Users/blaz/Desktop/LOTR/return-chapters/13.txt\":\n",
    "        break\n",
    "        \n",
    "#print(file)\n",
    "\n",
    "MountDoomFile = open(\"/Users/blaz/Desktop/LOTR/return-chapters/13.txt\", 'r')\n",
    "\n",
    "vect = CountVectorizer(max_features=10000, max_df=.15)\n",
    "MountDoom = vect.fit_transform(MountDoomFile)\n",
    "# Count Vectorizer removes all of the words that appear in at least 15% of the text\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=10, learning_method=\"batch\",\n",
    "                                max_iter=25, random_state=0)\n",
    "\n",
    "document_topics = lda.fit_transform(MountDoom)\n",
    "\n",
    "\n",
    "print(\"lda.components_.shape: {}\".format(lda.components_.shape))\n",
    "lda.components_.shape: (10, 10000)\n",
    "\n",
    "sorting = np.argsort(lda.components_, axis=1)[:, ::-1]\n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "\n",
    "mglearn.tools.print_topics(topics=range(4), feature_names=feature_names, \n",
    "                           sorting=sorting, topics_per_chunk=5, n_words=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Example JSON request\n",
    "#NOTE: Sentiment140 Polarity values: 0: negative, 2: neutral, 4: positive\n",
    "\n",
    "d = {'data':[{'text':'the titanic was ok'}, {'text':'this sucks'}]}\n",
    "d['data'].append({'text':\"Happy day!\"})\n",
    "\n",
    "r = requests.post('http://www.sentiment140.com/api/bulkClassifyJson?appid=blaz_j1@denison.edu', json=d)\n",
    "js = r.json()\n",
    "\n",
    "print(js['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Example of accessing the polarities\n",
    "\n",
    "for i in range(len(d['data'])):\n",
    "    print(\"Text:\", js['data'][i]['text'], \"\\nPolarity:\", js['data'][i]['polarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Segment list indexing examples\n",
    "\n",
    "#print(hobbit_segments[15]) ## -- chapter\n",
    "#print(hobbit_segments[0][0]) ## -- segments of chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Working with 'glob'\n",
    "\n",
    "path= '/Users/blaz/Desktop/LOTR/silmarillion-chapters'\n",
    "silm_chapters = []\n",
    "\n",
    "for file in sorted(glob.glob(os.path.join(path,'*.txt'))):\n",
    "    print(file)\n",
    "    f = open(file, 'r')\n",
    "    txt = f.read()\n",
    "    silm_chapters.append(txt)\n",
    "    \n",
    "print(len(silm_chapters))\n",
    "print(silm_chapters[len(silm_chapters)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using 'Counter'\n",
    "\n",
    "biglist = []\n",
    "for chapter in return_chapters_common:\n",
    "    for word in chapter:\n",
    "        biglist.append(word)\n",
    "        \n",
    "Counter(biglist).most_common(10)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
